from bs4 import BeautifulSoup
import pandas as pd

# EXTRACT LINK AND TEXT FROM HTML BACK IN LIST
# EXTRACT LINK AND TEXT FROM HTML BACK IN LIST

def extract_link_and_text(html):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –∏ —Å—Å—ã–ª–∫—É –∏–∑ HTML.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {"title": ..., "link": ..., "description": ...}
    """
    soup = BeautifulSoup(html, 'html.parser')
    a_tag = soup.find('a')

    if a_tag:
        return {
            "title": a_tag.get_text(strip=True),
            "link": a_tag.get('href', None),
            "description": None
        }
    else:
        return {
            "title": soup.get_text(strip=True),
            "link": None,
            "description": None
        }

# CLEAN HTML TAGS AND RETURN CLEAN TEXT
# CLEAN HTML TAGS AND RETURN CLEAN TEXT

def clean_html_tags(html_text):
    """–£–±–∏—Ä–∞–µ—Ç HTML-—Ç–µ–≥–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç"""
    soup = BeautifulSoup(html_text, "html.parser")
    return soup.get_text(separator="\n", strip=True)

# SAVING JSON DEFAULT LIST 
# SAVING JSON DEFAULT LIST 

def save_as_default(url, results):
    return {
        "url": url,
        "results": results  # –∑–¥–µ—Å—å —É–∂–µ —Å–ª–æ–≤–∞—Ä–∏, –Ω–µ –Ω—É–∂–Ω–æ –µ—â—ë —Ä–∞–∑ —á–∏—Å—Ç–∏—Ç—å
    }

    
    
# üìå –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ç–µ–≥–∞–º
# üìå –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ç–µ–≥–∞–º

def save_as_dynamic_tags(url, results, selector_input):
    """ –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ —Ç–µ–≥–∞–º, —É–±–∏—Ä–∞—è –Ω–µ–Ω—É–∂–Ω–æ–µ """
    selected_tags = [tag.strip() for tag in selector_input.split(",")]  # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–≥–∏ –∏–∑ input
    grouped_data = {tag: [] for tag in selected_tags}  # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É JSON

    for line in results:
        line = line.strip()

        # –§–∏–ª—å—Ç—Ä—É–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        if line.startswith(("–ü–∞—Ä—Å–∏–º", "‚úÖ", "üîπ", "‚ùå", "üõë")) or len(line) < 3:
            continue

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Å—ã–ª–∫–∏
        if "http" in line or "<a href=" in line:
            text, link = (line.split(" | ") if " | " in line else (line, ""))
            text, link = text.strip(), link.strip()
            if "a" in grouped_data:
                grouped_data["a"].append({"link_text": text, "url": link if link else None})
            continue

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫ –∫–∞–∫–æ–º—É —Ç–µ–≥—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫–æ–Ω—Ç–µ–Ω—Ç
        for tag in selected_tags:
            if line.startswith(f"{tag}:"):
                grouped_data[tag].append({"text": line.replace(f"{tag}:", "").strip()})
                break  # –ö–∞–∫ —Ç–æ–ª—å–∫–æ –Ω–∞—à–ª–∏ —Ç–µ–≥, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        else:
            continue  # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ç–µ–≥–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—ë

    return {
        "url": url,
        "data": {tag: grouped_data[tag] for tag in grouped_data if grouped_data[tag]}  # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–≥–∏
    }
    
# üìå –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –≤–∏–¥–µ —Å—Ç–∞—Ç–µ–π
# üìå –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –≤–∏–¥–µ —Å—Ç–∞—Ç–µ–π

def save_as_articles(url, results):
    articles = []
    current_article = {"title": "", "subtitles": [], "paragraphs": []}

    for line in results:
        if line.startswith("h1:"):
            if current_article["title"]:  # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç—å—é
                articles.append(current_article)
                current_article = {"title": "", "subtitles": [], "paragraphs": []}
            current_article["title"] = line.replace("h1:", "").strip()
        elif line.startswith("h2:"):
            current_article["subtitles"].append(line.replace("h2:", "").strip())
        else:
            current_article["paragraphs"].append(line.strip())

    if current_article["title"]:  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç–∞—Ç—å—é
        articles.append(current_article)

    return {
        "url": url,
        "articles": articles
    }

# üìå –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ —Å—Å—ã–ª–∫–∞–º    
# üìå –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ —Å—Å—ã–ª–∫–∞–º

def save_as_with_links(url, results):
    clean_results = [item for item in results if item.get("link")]
    return {
        "url": url,
        "data": clean_results
    }

# DICT WITH RESULTS AND JSON FORMAT FROM COMBOBOX
# DICT WITH RESULTS AND JSON FORMAT FROM COMBOBOX
    
def export_data_to_json(parsed_data, format_type):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ–¥ –≤—ã–±—Ä–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"""
    result_json = {}

    for url, results in parsed_data.items():
        if format_type == "default":
            result_json[url] = save_as_default(url, results)
        elif format_type == "group_by_tags":
            result_json[url] = save_as_dynamic_tags(url, results, "")
        elif format_type == "articles":
            result_json[url] = save_as_articles(url, results)
        elif format_type == "with_links":
            result_json[url] = save_as_with_links(url, results)
        else:
            raise ValueError(f"Unsupported format type: {format_type}")

    return result_json

# SAVING TO CSV TABLE
# SAVING TO CSV TABLE

def save_to_csv(parsed_data, file_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ CSV"""
    rows = []
    for url, data in parsed_data.items():
        for item in data:
            if isinstance(item, str):
                item = extract_link_and_text(item)  # üî• –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ —Å–ª–æ–≤–∞—Ä—å
            rows.append({
                "URL": url,
                "Title": item.get("title"),
                "Link": item.get("link"),
                "Description": item.get("description")
            })

    df = pd.DataFrame(rows)
    df.to_csv(file_path, index=False, encoding='utf-8-sig')

# SAVING TO EXCEL TABLE
# SAVING TO EXCEL TABLE

def save_to_excel(parsed_data, file_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ Excel"""
    rows = []
    for url, data in parsed_data.items():
        for item in data:
            if isinstance(item, str):
                item = extract_link_and_text(item)  # üî• –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ —Å–ª–æ–≤–∞—Ä—å
            rows.append({
                "URL": url,
                "Title": item.get("title"),
                "Link": item.get("link"),
                "Description": item.get("description")
            })

    df = pd.DataFrame(rows)
    df.to_excel(file_path, index=False, engine='openpyxl')
